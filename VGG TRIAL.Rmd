---
title: "CNN FDA"
author: "HITTESH KUMAR M"
date: "2024-11-19"
output: html_document
---
```{r}

# Load required libraries
library(keras)
library(tensorflow)
library(imager)
library(abind)

library(keras)

# Define the directories for your dataset
train_dir <- "/Users/hitteshkumarm/Desktop/COLLEGE/7th sem/FOUNDATIONAL ANALYTICS/PROJECT/Classification/train"
valid_dir <- "/Users/hitteshkumarm/Desktop/COLLEGE/7th sem/FOUNDATIONAL ANALYTICS/PROJECT/Classification/valid"
test_dir <- "/Users/hitteshkumarm/Desktop/COLLEGE/7th sem/FOUNDATIONAL ANALYTICS/PROJECT/Classification/test"

# Define the image data generators with rescaling (normalizing)
train_datagen <- image_data_generator(
  rescale = 1/255
)

valid_datagen <- image_data_generator(
  rescale = 1/255
)

test_datagen <- image_data_generator(
  rescale = 1/255
)

# Load images in batches using flow_from_directory
train_generator <- flow_images_from_directory(
  train_dir,
  generator = train_datagen,
  target_size = c(224, 224),
  batch_size = 32,
  class_mode = "binary"  # Binary classification (fire vs. nofire)
)

valid_generator <- flow_images_from_directory(
  valid_dir,
  generator = valid_datagen,
  target_size = c(224, 224),
  batch_size = 32,
  class_mode = "binary"
)

test_generator <- flow_images_from_directory(
  test_dir,
  generator = test_datagen,
  target_size = c(224, 224),
  batch_size = 32,
  class_mode = "binary"
)

# Load the pre-trained VGG16 model, excluding the top layers (we will add our own)
base_model <- application_vgg16(weights = "imagenet", include_top = FALSE, input_shape = c(224, 224, 3))

# Freeze the layers of the base model to avoid updating during training
freeze_weights(base_model)

# Add custom layers on top of VGG16
model <- keras_model_sequential() %>%
  base_model %>%
  layer_flatten() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")  # Sigmoid for binary classification

# Compile the model
model %>% compile(
  optimizer = optimizer_adam(),
  loss = "binary_crossentropy",
  metrics = c("accuracy")
)

# Model summary
summary(model)

# Train the model with early stopping
history <- model %>% fit_generator(
  train_generator,
  validation_data = valid_generator,
  epochs = 20,
  steps_per_epoch = 10,  # Number of batches per epoch
  validation_steps = 50,  # Number of validation batches
  callbacks = list(callback_early_stopping(monitor = "val_loss", patience = 4))
)



```

```{r}

# Evaluate the model on test data
score <- model %>% evaluate(
  test_generator,
  steps = 10 # Adjust based on your test dataset and batch size
)

# Print results
cat("Test loss:", score[1], "\n")
cat("Test accuracy:", score[2], "\n")


```
